 % use the "wcp" class option for workshop and conference
 % proceedings
 %\documentclass[gray]{jmlr} % test grayscale version
 %\documentclass[tablecaption=bottom]{jmlr}% journal article
 \documentclass[tablecaption=bottom,wcp]{jmlr} % W&CP article

 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

 %\usepackage{rotating}% for sideways figures and tables
 %\usepackage{longtable}% for long tables

 % The booktabs package is used by this sample document
 % (it provides \toprule, \midrule and \bottomrule).
 % Remove the next line if you don't require it.
\usepackage{booktabs}
\usepackage{comment}
\usepackage{multirow}
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} % newer version
 %\usepackage{siunitx}

% flow chart
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}


 % The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}% remove this in your real article
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

 % Define an unnumbered theorem just for this sample document for
 % illustrative purposes:
\theorembodyfont{\upshape}
\theoremheaderfont{\scshape}
\theorempostheader{:}
\theoremsep{\newline}
\newtheorem*{note}{Note}

 % change the arguments, as appropriate, in the following:
\jmlrvolume{}
\jmlryear{2015}
\jmlrsubmitted{\today}
\jmlrpublished{\today}
\jmlrworkshop{9.520 Project Report} % W&CP title

 % The optional argument of \title is used in the header
 % If you want to force a line break within the title use \titlebreak instead of \\
 % but use sparingly
\title{{\tt rls\_dual\_mkl}: A PFBS-based Implementation for Multiple Kernel Learning}

% macros from Bob Gray
\usepackage{"./macro/GrandMacros"}
\usepackage{"./macro/Macro_BIO235"}

 % Anything in the title that should appear in the main title but 
 % not in the article's header or the volume's table of
 % contents should be placed inside \titletag{}

 %\title{Title of the Article\titletag{\thanks{Some footnote}}}


 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % \thanks must come after \Name{...} not inside the argument for
 % example \Name{John Smith}\nametag{\thanks{A note}} NOT \Name{John
 % Smith\thanks{A note}}

 % Anything in the name that should appear in the title but not in the 
 % article's header or footer or in the volume's
 % table of contents should be placed inside \nametag{}

 % Two authors with the same address
  \author{\Name{(Jeremiah) Zhe Liu} \Email{zhl112@mail.harvard.edu}\\
   \addr Department of Biostatistics\\
   Harvard School of Public Health\\
   Boston, MA, 02115}

\begin{document}

\maketitle
\vspace*{-4em}
\tableofcontents


%\begin{abstract}
%This is the abstract for this article.
%\end{abstract}
%\begin{keywords}
%List of keywords
%\end{keywords}

\newpage
\section{Introduction}
\subsection{Multiple Kernel Learning Problem}

Multiple kernel learning (MKL) (\citet{jordan-mkl-2004}) is the process of finding an optimal kernel
from a prescribed (convex) set $\Ksc$ of basis kernels, for learning a real-valued function by regularization. In this work, we consider a RKHS $\Hsc = \Hsc_1 \oplus \Hsc_2 \dots \oplus \Hsc_M$ with reproducting kernel $\bk \in \Ksc = \{\sum_{i=1}^M c_i\bk_i | ( c_i \geq 0 \forall i) \wedge \sum_{i=1} c_i = 1 \}$ such that $f = \sum_{i=1}^M f_i, f_i \in \Hsc_i$. By \citet{micchelli-convex-2005}, the problem of multiple kernel learning corresponds to find $f^*$ such that:
\begin{align*}
\arg \min_{f \in \Hsc} \Big\{
\frac{1}{n} \sum_{i=1}^n Q(\sum_{j=1}^M f_j(\bx), \by) + 
\tau g \big( (\sum_{j=1}^M ||f_j||_\Hsc )^2  \big)
\Big\}
\end{align*}

\citet{rosasco-prox-2009} generalized above problem by taking $Q$ to be square loss, $g(.) = \sqrt{.}$ and also impose L1 regularization, leading to the elastic-net-regulated problem:
\begin{align}\label{eq:primal}
\arg \min_{f \in \Hsc} \Big\{
\frac{1}{n} \sum_{i=1}^n (\sum_{j=1}^M f_j(x_i) - y_i)^2 + 
\mu \sum_{j=1}^M ||f_j||^2_\Hsc +
2\tau \sum_{j=1}^M ||f_j||_\Hsc \Big\}
\end{align}

\subsection{Iterative PFBS Algorithm} \label{sec:PFBS}
By Theorem 1 of \citet{rosasco-prox-2009}, since the penalty function is lower semicontinuous, coercive, convex and one-homogenous,   solution to problem \ref{eq:primal} $f^*$ is the unique fixed point of the the contractive mapping with step size $\sigma$:
\begin{align*}
\Tsc_\sigma(f) &= (\bI - \pi_{\frac{\tau}{\sigma}K}) 
\big(f - \frac{1}{2\sigma} 
\nabla_f [ \frac{1}{n}||f - y||^2 ] \big)
\end{align*}
where $\pi_{\frac{\tau}{\sigma}K}(g)$ is a project operator which project $g$ to $\Hsc' = \{f \in \Hsc \big|\;   ||f_j||_{\Hsc_j} \leq \frac{1}{\tau/\sigma} \; \forall j \}$, or more rigorously:
$$ \pi_{\frac{\tau}{\sigma}K}(g) = \frac{\tau}{\sigma} v, \qquad \mbox{where } v = \arg\min_{v\in \Hsc, ||v_j||\leq 1} ||\frac{\tau}{\sigma}v - g||_\Hsc^2$$

Above mapping can also be written in terms of Kernel matrices by  generalizing representer theorem and write $f_j^*(x) = \sum_{i=1}^n \alpha_{ji}^Tk_j(x_i, x) = \balpha_j^T\bk_j(x)$, where $\balpha_j$ and $\bk_j(x)$ are $n \times 1$ vectors. Further, if denote:
\begin{align*}
\balpha_{Mn \times 1} &= (\balpha_1, \dots, \balpha_M)^T\\
\bk(x)_{Mn \times 1} &= (\bk_1(x)^T, \dots, \bk_M(x)^T)^T\\
\bK_{Mn \times Mn} &= 
\begin{bmatrix}
\bK_1 & \dots & \bK_M \\
\vdots & \ddots & \vdots \\
\bK_1 & \dots & \bK_M
\end{bmatrix}
, \mbox{where }\bK_i = \bk_i(.)\bk_i(.)^T\\
\by_{Mn \times 1} &= (y_{n \times 1}^T, \dots, y_{n \times 1}^T)^T
\end{align*}
The contraction mapping can be written as:
\begin{align*}
\Tsc_\sigma(f) &= (\bI - \pi_{\frac{\tau}{\sigma}K}) 
\big( \big[ 
(1-\frac{\mu}{\sigma})\balpha - \frac{1}{\sigma n}(\bK\balpha - \by)\big]^T \bk  \big) \qquad \mbox{where}
\numberthis \label{eq:PFBS}
\\
\pi_{\frac{\tau}{\sigma}K}(g)_j &= 
min\{1, \frac{||g_j||_{\Hsc_j}}{\tau/\sigma}\} * \frac{g_j}{||g_j||_{\Hsc_j}} = 
min\{1, \frac{\sqrt{\balpha^T_j\bK_j\balpha_j}}{\tau/\sigma}\} * \frac{\balpha_j^T\bk_j}{\sqrt{\balpha^T_j\bK_j\balpha_j}}
\end{align*}
Thus the projection $\bI - \pi_{\frac{\tau}{\sigma}K}$ corresponds to the soft-thresholding operator for $\balpha_j$:
\begin{align*}
\bS_{\frac{\tau}{\sigma}}(K, \balpha)_j &= 
\frac{\balpha_j^T}{\sqrt{\balpha^T_j\bK_j\balpha_j}}
(\sqrt{\balpha^T_j\bK_j\balpha_j} - \frac{\tau}{\sigma})_+
\end{align*}
Above discussions lead to below algorithm:
\begin{algorithm}[htbp]
\floatconts
  {alg:gauss}%
  {\caption{MKL Algorithm}}%
{%
\begin{enumerate}
  \item[] \textbf{set} $\balpha^0=\bzero$
  \item[] \textbf{for} $p=1$ to ${\tt MAX\_ITER}$ \textbf{do}
  \begin{enumerate}
  \item[] $\balpha_0^{p} = (1-\frac{\mu}{\sigma})\balpha^{p-1} - \frac{1}{\sigma n}(\bK\balpha^{p-1} - \by) $
  \item[] $\balpha^p = 
  \bS_{\frac{\tau}{\sigma}}(K, \balpha_0^{p})$
  \end{enumerate}
  \item[] \textbf{end for}
  \item[] \textbf{return} $f^{\tt MAX\_ITER} = (\balpha^{\tt MAX\_ITER})^T\bk$
\end{enumerate}
}%
\end{algorithm}

\subsection{Implementation Detail}

\subsubsection{Block-wise Update}

Notice that in (\ref{eq:PFBS}), $\Tsc_\sigma$ updates $\balpha$ by group, it is thus possible to write $\Tsc$ at $p^{th}$ step as:

\begin{align*}
\Tsc^p_\sigma &= [\Tsc^p_{\sigma, 1}, \Tsc^p_{\sigma, 2}, \dots, \Tsc^p_{\sigma, M}] 
\qquad \mbox{with} \qquad
\Tsc^p_{\sigma, j} = 
\textbf{S}_{\frac{\tau}{\sigma}}
\Big(K, \balpha_0 \Big)_j \\
\balpha_0 &= 
(1-\frac{\mu}{\sigma})\balpha^{p-1}_j - 
\frac{1}{n \sigma} * \bepsilon^{p-1}  
\qquad \mbox{where } 
\bepsilon^{p-1}= (\sum_{j=1}^M \bK_j \balpha^{p-1}_j - y)
\end{align*}
by using above method we are able to avoid working directly with the $Mn \times Mn$ matrix $\bK$ (as defined earlier in section \ref{sec:PFBS}), which led to reduced memory cost \footnote{$O(Mn^2)$ instead of $O(M^2n^2)$} and reduced difficulty in selecting stepsize and regularization parameters.

\subsubsection{Choice of Stepsize}

Based on \citet{jordan-mkl-2004}, it can be shown that a suitable choice of $\sigma$ is $\sigma = \frac{1}{4}(a*L_{min} + b*L_{max}) + \mu$, where $(b, a)$ denotes the lower/upper bound on the eigenvalue of $\bK$, and $(L_{min}, L_{max})$ denotes the lower/upper bound of $\nabla^2 Q(\textbf{f}, \by)$.

In the context where  Q is square loss (i.e. $\nabla^2_{\textbf{f}} Q(\textbf{f}, \by) = 2$), we have:
\begin{align*}
\sigma = \frac{1}{2}(a + b) + \mu
\end{align*}

A naive choice of $a$ would be the largest eigenvalue of $\bK_{nM \times nM}$, which not only is computationally expensive but also leads to overly slow convergence. In pactice, if denote the maximum eigenvalue of each kernel matrix $K_j$ to be $a_j$, it is found that setting $a$ to be $\underset{j \in \{1, .., M\}}{\max} (a_j)$ is suffice to guarantee convergence.  This is because the mapping $\balpha^{p-1} \mapsto \balpha_0^{p}$ can be written as:
$$
\balpha_0 = 
(1-\frac{\mu}{\sigma})\balpha^{p-1}_j - 
\frac{1}{n} * \sum_{j=1}^M \frac{1}{\sigma}
(\bK_j \balpha^{p-1}_j - \frac{y}{n})
$$
As shown, in $\balpha^{p-1} \mapsto \balpha_0^{p}$ we actually updated $\balpha^{p-1}$ M times, with step size $\frac{1}{\sigma}(\bK_j \balpha^{p-1}_j - \frac{y}{n})$ in each step. It is thus sufficient to find a $a$ that properly scale the magnitude of all $||\bK||$, leading natually to the choice $a = \underset{j \in \{1, .., M\}}{\max} (a_j)$.


\subsubsection{Effect of $\mu$ and $\tau$}
The convergence of the aforementioned procedure is guaranteed by Banach Fixed Point theorem, given proper choice of $\sigma$. 

\citet{Zou-2005}

\citet{rosasco-prox-2009}


Contraction mapping

$||(1 - \frac{\mu}{\sigma})\bI - \frac{1}{\sigma n} \bK ||$\\

Continuation strategy.

$\frac{\tau}{\sigma}_{max} = \frac{||y||^2}{||\bK||}$



\section{Software Structure and Usage}

% Define block styles
\tikzstyle{opt} = [rectangle, draw, fill=green!20, 
    text width=6em, text centered, minimum height=3em, 
    node distance=2cm]
\tikzstyle{proc} = [rectangle, draw, fill=blue!20, 
    text width=6em, text centered, rounded corners, minimum height=3em, node distance=3cm]
\tikzstyle{opt_text} = [rectangle, 
    text width=8em, text centered, minimum height=3em,
    node distance=2cm]    
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=2em, node distance=3cm]

\begin{tikzpicture}[node distance = 1cm, scale=0.9, every node/.style={scale=0.9}]
    % Place nodes
    \node [cloud] (input) 
    {$\bX_{n \times d}$, $\by_{n \times 1}$};    
    \node [opt_text, above of=input] (text1)  
    {User Specified \\ OPT};
    \node [opt_text, below of=input] (text2)  
    {Routine Produced\\ OPT};          
    %
    \node [proc, right of=input] (kernel) {{\tt kernel\_mkl}};    
    \node [opt, above of=kernel] (kernel_type)  
    {{\tt mkl.}\\{\tt kernel\_type}};
    \node [opt, below of=kernel] (K_mkl)  
    {{\tt kernel.K\_mkl}};        
    %
    \node [proc, right of=kernel] (paramsel) 
    {{\tt paramsel\_} \\ {\tt homkl}};    
    \node [opt, above of=paramsel] (par) 
    {{\tt mkl.npar/}\\{\tt mkl.parrange}};
    \node [opt, below of=paramsel] (paramsel_out) 
    {{\tt paramsel.} \\ {\tt par\_mkl}};
    % 
    \node [proc, right of=paramsel] (rls) 
    {{\tt rls\_dual\_mkl}};    
    \node [opt, above of=rls] (mkl_par) 
    {{\tt mkl.par}};   
    \node [opt, below of=rls] (C_mkl) 
    {{\tt rls.C\_mkl}};   
    % 
    \node [proc, right of=rls] (pred) 
    {{\tt pred\_dual\_mkl}};    
    % 
    \node [proc, right of=pred] (perf) 
    {{\tt perf}};        
    %%% Draw edges
    \path [line] (input) -- (kernel);
    \path [line] (kernel_type) -- (kernel);    
    \path [line] (kernel) -- (K_mkl);            
    \path [line] (K_mkl) -- (paramsel);        
    \path [line] (par) -- (paramsel);            
    \path [line] (paramsel) -- (paramsel_out);            
    \path [line] (paramsel_out) -- (rls);                
    \path [line, dashed] (mkl_par) -- (rls);                
    \path [line] (rls) -- (C_mkl);                
    \path [line] (C_mkl) -- (pred);                
    \path [line] (pred) -- (perf);                
\end{tikzpicture}


\begin{enumerate}
\item Parameter Specification
\begin{enumerate}
\item [$\bullet$] {\tt util/gurls\_defopt\_mkl}
\end{enumerate}
\item Kernel Generation
\begin{enumerate}
\item [$\bullet$] {\tt kernel/kernel\_mkl}
\end{enumerate}
\item Parameter Selection
\begin{enumerate}
\item [$\bullet$] {\tt kernel/paramsel\_homkl}
\item [$\bullet$] {\tt util/paramsel\_L1ratioguesses}
\end{enumerate}
\item Optimization
\begin{enumerate}
\item [$\bullet$] {\tt optimizers/rls\_dual\_mkl}
\item [$\bullet$] {\tt optimizers/rls\_dual\_mkl\_pfbs}
\item [$\bullet$] {\tt util/ConsoleProgressBar}
\item [$\bullet$] {\tt summary/plot\_mkl\_L1path}
\end{enumerate}
\item Prediction
\begin{enumerate}
\item [$\bullet$] {\tt kernel/predkernel\_traintest}
\item [$\bullet$] {\tt optimizers/pred\_dual\_mkl}
\end{enumerate}
\item Performance assessment
\begin{enumerate}
\item [$\bullet$] {\tt perf/perf\_rmsestd}
\end{enumerate}
\end{enumerate}

\section{Example}

\footnote{LIBSVM Data Repository:
\url{'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html'}}

\newpage
\bibliography{./report}

\end{document}